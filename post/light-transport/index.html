<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <title>Notes on Light Transport - Old and New</title>
  <meta property="og:title" content="Notes on Light Transport - Old and New" />
  <meta name="twitter:title" content="Notes on Light Transport - Old and New" />
  <meta name="description" content="In this note we will approach light transport in two directions. The first is from the perpective of simulation and modelling, the other is from the perspective of vision and inference. We will see different forms of light transport equation that connect these two aspects. I hope this note will help who wants to understand light transport in a mostly high-level fashion.
1. A conceptual tour in physically based rendering To some extent, creating photo-realistic renderings means understanding our visual perceptions, which are likely to be evolving as our knowledge advances, and accordingly we will make improvements to the corresponding rendering models.">
  <meta property="og:description" content="In this note we will approach light transport in two directions. The first is from the perpective of simulation and modelling, the other is from the perspective of vision and inference. We will see different forms of light transport equation that connect these two aspects. I hope this note will help who wants to understand light transport in a mostly high-level fashion.
1. A conceptual tour in physically based rendering To some extent, creating photo-realistic renderings means understanding our visual perceptions, which are likely to be evolving as our knowledge advances, and accordingly we will make improvements to the corresponding rendering models.">
  <meta name="twitter:description" content="In this note we will approach light transport in two directions. The first is from the perpective of simulation and modelling, the other is from the perspective of vision and inference. We will see …">
  <meta name="author" content=""/>
  <meta property="og:site_name" content="Old and New" />
  <meta property="og:url" content="http://example.org/post/light-transport/" />
  <meta property="og:type" content="article" />
  <meta name="twitter:card" content="summary" />
  <meta name="generator" content="Hugo 0.55.5" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha256-uXNHy6FK52Pb83SmU45mVAg7YECmr9Lwwu1zOz31j5c=" crossorigin="anonymous" />

  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="/js/script.js"></script>
  <script src="/js/custom.js"></script>
  <script defer src="/js/fontawesome.js"></script>
</head>

<body>

<header class="site-header">
  <nav class="site-navi">
    <h1 class="site-title"><a href="/post/">Old and New</a></h1>
    <ul class="site-navi-items">
      <li class="site-navi-item-about"><a href="/" title="About">About</a></li>
    </ul>
  </nav>
</header>
<hr class="site-header-bottom">
<script src='http://cdn.mathjax.org/mathjax/latest/MathJax.js' type='text/javascript'>
    MathJax.Hub.Config({
      "HTML-CSS": {
        scale: 94
      }
    });
    MathJax.Hub.Config({
      "HTML-CSS": {
        preferredFont: "TEX"
      }
    });
    MathJax.Hub.Config({
     extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
     },
     "HTML-CSS": { availableFonts: ["TeX"] }
    });
    </script >

  <div class="main" role="main">
    <article class="article">
      
      
      <h1 class="article-title">Notes on Light Transport</h1>
      
      <hr class="article-title-bottom">
      <ul class="article-meta">
        <li class="article-meta-date"><time>June 12, 2014</time></li>
      </ul>
      
<aside class="toc">
  <nav id="TableOfContents">
<ul>
<li><a href="#1-a-conceptual-tour-in-physically-based-rendering">1. A conceptual tour in physically based rendering</a></li>
<li><a href="#2-monte-carlo-path-tracing">2. Monte-Carlo path tracing</a>
<ul>
<li><a href="#2-1-rendering-direct-illumination">2.1. Rendering direct illumination</a>
<ul>
<li><a href="#variance-reduction-in-direct-illumination-calculation">Variance reduction in direct illumination calculation</a></li>
</ul></li>
<li><a href="#2-2-the-path-tracing-algorithm">2.2. The path tracing algorithm</a></li>
<li><a href="#2-3-bidirectional-methods">2.3. Bidirectional methods</a></li>
</ul></li>
<li><a href="#3-image-space-stationary-light-transport">3. Image space stationary light transport</a>
<ul>
<li><a href="#3-1-nystr-ouml-m-light-transport">3.1. Nystr&ouml;m light transport</a></li>
<li><a href="#3-2-light-transport-probing">3.2. Light transport probing</a></li>
<li><a href="#3-3-transient-light-transport">3.3. Transient light transport</a></li>
</ul></li>
</ul>
</nav>
</aside>
      

<p>In this note we will approach light transport in two directions. The first is from the perpective of simulation and modelling, the other is from the perspective of vision and inference. We will see different forms of <em>light transport equation</em> that connect these two aspects. I hope this note will help who wants to understand light transport in a mostly high-level fashion.</p>

<h1 id="1-a-conceptual-tour-in-physically-based-rendering">1. A conceptual tour in physically based rendering</h1>

<p>To some extent, creating photo-realistic renderings means understanding our visual perceptions, which are likely to be evolving as our knowledge advances, and accordingly we will make improvements to the corresponding rendering models. Today, physically accurate reconstruction of an image about a scene requires a good understanding of the interactions between the scene geometry, scene materials and the light sources.
Although the underlying physical principles of these interactions can be very complicated and computation unfriendly, it turns out fine to abstract away many physical properties, for the purpose of producing renderings that are photo-relistic to us.</p>

<p>In most cases, we can simplify the model by abstracting away the wave-ish properties of light, and thus our model cannot directly produce e.g. polarization, chromatic dispersion, diffraction etc., but these properties can be taken into consideration afterwards via the <em>principle of superposition</em>
(here we need to assume linear optics, while it can be false if e.g. the light propagates in a media that can interact with the electro-magenetic field of light, or under extreme gravity).
In this way, we &ldquo;modulize&rdquo; the simulation of light-scene interaction.
At the top level abstraction, we can think of light as &ldquo;photons&rdquo; with their frequency unspecified.
The image we render represents the spatial distribution of photon density at the very moment we record it.
In other words, we record, at each visible scene point and at a particular instant, how many photons are sent out towards our perspestive.</p>

<p>Since light travels so fast compared to ordinary camera exposure time or our perception, any real image taken, or the vision as we perceive should represent an equilibrium state of the photon transportation within the scene (of course, this does not apply to scenes at astronomy scale, or cameras that can capture transient states of light).
This equilibrium is characterized nicely by the <em>light transport equation</em>, a.k.a. <em>rendering equation</em>, proposed by Kajiya in 1986.</p>

<p>To dive into the details, we will need to find appropriate differential radiometric quantities around a scene point $p$. First let us consider:</p>

<ul>
<li><em>Irradiance</em>: The number of photons arriving at $p$ per unit area per unit time.</li>
<li><em>Radiant exitant</em>: The number of photons leaving from $p$ per unit area per unit time.</li>
</ul>

<p>These two important quantities describe the incoming and outgoing photon density with respect to area and time. However, these quantities are irrespective to <em>light direction</em>.
Imagine a device that can receive and count the number of photons emitted from $p$ per receiving area per unit time.
If we use this device to measure the radiant exitant around $p$, we will get different results if the normal vector of our device&rsquo;s receiving plane is facing towards different directions than to $p$.
This is formally known as <em>Lambert&rsquo;s Law</em>: we will have a cosine term multiplied to the meansured density to compensate the the inverse cosine term multiplied to the receiving area, so as to keep the number of photons constant.
And because the measuring density is decreased, we will perceive a darkening effect, even though only the viewing direction is changed.</p>

<figure>
<img src="figs/lambert_law.png" alt="lambert_law" width=250 style="padding-bottom:0.5em;"/> 
<figcaption>Fig.1 Lambert's Law. Figure is taken from the <b>pbrt</b> book.</figcaption>
</figure>

<p>Thus, we will consider finer differential radiometric quantities that take directions into account.</p>

<ul>
<li><em>Incident radiance</em>: The number of photons arriving at $p$ per solid angle (i.e. standard measure on the unit sphere $\mathbf{S}^2$) per unit time.</li>
<li><em>Exitant radiance</em>: The number of photons leaving from $p$ per solid angle per unit time.</li>
</ul>

<p>These two quantities are closely related to the irradiance and radiant exitant. If we integrate incident radiance (or exitant radiance) over a unit sphere centered at $p$, we will get the irradiance (or resp. radiant exitant).
In particular, if no light can pass through $p$ because of its material property, then it is equivalent to integrate over the unit half-sphere $\mathbf{H}^2$ above the tangent plane at $p$.</p>

<figure>
<img src="figs/radiance_def.png" alt="radiance_def" width=300 style="padding-bottom:0.5em;"/> 
<figcaption>Fig.2 Incident radiance and exitant radiance. The figure is taken from the <b>pbrt</b> book.</figcaption>
</figure>

<p>The relation between incident radiance and exitant radiance at $p$ is dictated by the material property at $p$.
Imagine a beam of light coming from direction $\omega<em>i \in \mathbf{S}^2$ arriving at $p$, in other words, a incident radiance $L</em>{i}(p, \omega_{i})$.
The outgoing light from $p$ due to this light beam is a distribution of exitant radiance over the unit sphere centered at $p$.
Denote the exitant radiance in direction $ \omega<em>o$ as $L</em>{o}(p, \omega<em>{o})$.
For simplicity, we assume a certain ratio of $L</em>{i}(p, \omega<em>{i})$ is reflected or transmitted in the form of $L</em>{o}(p, \omega_{o})$.
Because of Lambert&rsquo;s Law, we also expect a uniform decrease in this ratio if the angle between $\omega_i$ and the surface normal at $p$ is large.
We can thus normalize this effect out, which leads us to the <em>bidirectional scattering distribution function (BSDF)</em></p>

<p>$$
f(p, \omega_o, \omega<em>i) = \frac{L</em>{o}(p, \omega<em>{o})}{L</em>{i}(p, \omega_{i})|\cos(\theta_i)|},
$$</p>

<p>where $\theta_i$ is the angle between $\omega_i$ and the surface normal $\mathbf{n}$. We will assume this quantity depends only on the material. For examples,
* For perfect specular reflection, the photons from $\omega_i$ are simply copied to its mirrored direction $\omega_o$, and so $f$ is a delta distribution for fixed $\omega_i$.
* For glossy reflection, $f$ is more spread-out within a cone of directions than a perfect specular reflection.
* For perfect Lambertian reflection, $f$ is a uniform distribution over the half sphere $\mathbf{H}^2$.
* For a <em>black body</em>, $f$ is simply zero.</p>

<p>Now we are ready to derive the light transport equation. For a scene point $p$, we must have conservatoin of energy. This means the difference in the radiant exitant and irradiance must equate the amount of photons emitted minus the amount absorbed per unit time. Moving the irradiance to the right hand side, and reformulate this equality in terms of incident radiance $L_i(p, \omega_i)$, exitant radiance $L_o(p, \omega_o)$, emitted radiance $L_e(p, \omega_o)$ in direction $\omega_o$ and the BSDF, we have</p>

<p>$$
L_o(p, \omega_o) = L_e(p, \omega<em>o) + \int</em>{\mathbf{S}^2}
                f(p, \omega_o, \omega_i)L_i(p, \omega_i)|\cos(\theta_i)| \, d\omega_i.
$$ (1)</p>

<p>This light transport equation will be the central theme throughout this note. We will next turn to the evalation of the solution of this integral equation given scene geometry, materials (in terms of BSDFs), and light sources through Monte-Carlo integration.</p>

<h1 id="2-monte-carlo-path-tracing">2. Monte-Carlo path tracing</h1>

<p>Consider a non-light-source point $p$. The point&rsquo;s exitant radiance in direction $\omega_o$ is a result from the incident radiance at $p$, which in turn can possibly come from all other scene points.
We can already sense a recursive process going on: most points are first lit by light sources, and then all the points lit each other, and then all the points lit each other again, and it goes on like this literally infinite number of times.
The result after the first &ldquo;light bounce&rdquo; is called <em>direct illumination</em>, and the result afterwards is called <em>indirect illumination</em>. Indirect illumination accounts a great deal in the realism of a rendered image.</p>

<p>Thus the final rendered image is a superposition of first bounce image, second bounce image, third bounce image, etc.
In practice we may terminate before a prescribed maximum level of bounces, since the contribution of each bounce is diminishing.</p>

<p>The idea is clearer if we consider the path integral formulation of light transport.
We denote exitant radiance sent from $p&rsquo;$ to $p$ by $L(p&rsquo; \to p)$. The BSDF term as $f(p&rdquo;\to p&rsquo; \to p)$.
Here we no longer integrate over $\mathbf{S}^2$ but the <strong>scene surface</strong> $A$.
Due to this reason, we will need to change the cosine term to a &ldquo;geometry term&rdquo;.
To change the integration domain to the entire scene, we use the change of variable formula in integration.</p>

<figure>
<img src="figs/integration_domain.png" alt="integration_domain" width=300 style="padding-bottom:0.5em;"/> 
<figcaption>Fig.3 Change of variable from the unit sphere measureto the scene surface measure. The figure is taken from the <b>pbrt</b> book.</figcaption>
</figure>

<p>The Jacobian of this change of variable is $|\cos(\theta&rdquo;)|/|p&rdquo; - p|^2$, where $\theta&rdquo;$ is the angle between the ray $p\to p&rdquo;$ and the normal vector at $p&rdquo;$. The geometry term becomes</p>

<p>$$G(p&rsquo;, p&rdquo;) = V(p&rsquo;, p&rdquo;)\frac{|\cos(\theta&rsquo;)||\cos(\theta&rdquo;)|}{|p&rdquo; - p&rsquo;|^2}$$</p>

<p>where $V(p&rsquo;, p&rdquo;)$ is $1$ if $p&rdquo;$ is visible from $p&rsquo;$ and $0$ otherwise. Then the light transport equation writes</p>

<p>$$
L(p&rsquo;\to p) = L_e(p&rsquo;\to p) + \int_A f(p&rdquo;\to p&rsquo; \to p) L(p&rdquo; \to p&rsquo;) G(p&rsquo;, p&rdquo;) dA(p&rdquo;)
$$ (2)</p>

<p>Note that if we consider all pairs of $(p, p&rsquo;)$ and record the radiance in an &ldquo;array&rdquo; as $L$, the integration term above can be seen as a linear operator on $L$.
Hence, in operator form (2) is</p>

<p>$$
L = L_e + \mathcal{A}L
$$
where $L_e$ is the <em>first bounce radiance</em>, and the operator $\mathcal{A}$ is called the <em>light transportation matrix</em>. Given $L_e$, we can solve for the final radiance by inverting the operator $(I - \mathcal{A})$, which has a <em>Neumann series</em> expansion</p>

<p>$$ \begin{aligned}
L &amp; = (I-\mathcal{A})^{-1}L_e
 &amp; = L_e + \mathcal{A}L_e + \mathcal{A}^2L_e + \mathcal{A}^3L_e + \cdots
\end{aligned}
$$
which agrees well with our disucssion about the first bounce, second bounce, etc. images.</p>

<p>We now go on to describe the Monte-Carlo integration for the first bounce image, and then describe the procedure to compute the result for a many-bounce image.</p>

<h2 id="2-1-rendering-direct-illumination">2.1. Rendering direct illumination</h2>

<p>Assume a simple pinhole camera model. To render the image as taken by this camera, we need to connect pixels on the image plane to each scene point and record the incident radiance on the image plane, i.e. a camera ray. Suppose the pixel is in the direction $\omega_o$ at a scene point $p$.
The main job is to compute</p>

<p>$$
L_o(p, \omega<em>o) = \int</em>{\mathbf{S}^2} f(p, \omega_o, \omega_i)
                   L_d(p, \omega_i) |\cos(\theta_i)|d\omega_i
$$
where $L_d$ is the incident radiance directly from the light sources. $L_d$ in path integral framework is</p>

<p>$$
 V(p&rsquo;,p)\frac{|\cos(\theta&rsquo;)|}{|p&rsquo;-p|^2} L(p&rsquo;\to p)
$$
where $p&rsquo;$ is a point from the light source in the $\omega_i(p)$ direction and $L(p&rsquo;\to p)$ is the simply number of photons emitted at $p&rsquo;$ towards $p$ per unit time. Note that we should integrate this quantity over the entire light source surface $A_L$.</p>

<p>Monte-Carlo integration evaluates the integral by taking random samples $p&rsquo; \in A_L$ with probability $P(p&rsquo;)$, evaluating the integrand, and doing harmonic average.</p>

<p>$$
\widehat{L_o}(p, \omega<em>o) = \sum</em>{j} \frac{1}{P(p&rsquo;)}
              f(p&rsquo;\to p \to \text{cam})
                   V(p&rsquo;,p)\frac{|\cos(\theta_o)||\cos(\theta&rsquo;)|}{|p&rsquo;-p|^2}                          L(p&rsquo;\to p)
$$
where $\theta_o$ is the angle between the light ray $p&rsquo;p$ and the surface normal at $p$, and $P(p&rsquo;) = P(\omega_i(p)) \frac{|\cos(\theta&rsquo;)|}{|p&rsquo;-p|^2}$ is the transformed probability. Hence in fact the above equation can be simplified to</p>

<p>$$
\widehat{L_o}(p, \omega<em>o) = \sum</em>{j} \frac{1}{P(\omega_i)}
              f(p, \omega_o, \omega_i)
                   V(p&rsquo;,p)|\cos(\theta_o)|L(p&rsquo;\to p) .
$$ (3)
One easily verifies that this estimation is unbiased:</p>

<p>$$
L_d(p, \omega_i) = \mathbb{E}\big (\widehat{L_d}(p, \omega_i) \big)
$$</p>

<p>The question is then how to make this estimate converge faster, or equivalently how to make the variance of this estimate smaller.</p>

<h3 id="variance-reduction-in-direct-illumination-calculation">Variance reduction in direct illumination calculation</h3>

<p>A course in statistical computing tells us that the variance is smaller if the probability distribution $P$ resembles the integrand.
Since we can cheaply evaluate the integrand but too expensive to compute a joint probability distribution of light and BSDF out of it, we have to cleverly choose a probability distribution so that it is large when the integrand is large. In this regard, <strong>pbrt</strong> uses a <em>power heuristic</em> for <em>multiple importance sampling</em> and stratefied sampling.</p>

<h2 id="2-2-the-path-tracing-algorithm">2.2. The path tracing algorithm</h2>

<p>To illustrate the idea we show how to compute the second bounce image.
We now want to trace a ray from a scene point $p$ to another general scene point $p&rsquo;$, and compute the incident radiance of the ray from $p&rsquo;$ to $p$, given the exitant radiance distribution at $p&rsquo;$ due to direct lighting.
The latter can be Monte-Carlo-ly esitmated by re-tracing a ray from $p&rsquo;$ to a point from the light sources.</p>

<p>The overall process is very similar to that of the first bounce. We sample a direction $\omega_i(p)\in\mathbf{S}^2$ at $p$ with probability $P(\omega_i(p))$.
We then find a closest interection point in the $\omega_i(p)$ direction, namely $p&rsquo;$, that will give a exitant radiance direction $\omega_o(p&rsquo;)$ at $p&rsquo;$.
Suppose at $p&rsquo;$ the exitant radiance is computed as $L(p&rsquo;\to p) = L_o(p&rsquo;, \omega_o)$, and suppose the camera ray to $p$ is in direction $\omega_o(p)$.
It follows that the exitant radiance at $p$  due to $L_o(p&rsquo;, \omega_o)$ is</p>

<p>$$ \begin{aligned}
L_o(p, \omega<em>o(p)) &amp; = \int</em>{\mathbf{S}^2} f(p, \omega_o(p), \omega_i(p))L_i(p, \omega_i(p))|\cos(\theta_i(p))| d\omega<em>i   <br />
&amp; = \int</em>{\mathbf{S}^2} f(p, \omega_o(p), \omega_i(p)) |\cos(\theta<em>i(p))|\times <br />
&amp; ~~~~~~~~ \int</em>{A_{L}}f(p&rdquo;\to p&rsquo; \to p))L(p&rdquo;\to p)G(p&rdquo;, p&rsquo;) ~d\omega<em>i(p) dA</em>{L}(p&rdquo;) <br />
&amp; \approx \sum_{j, p&rdquo;} \frac{1}{P(\omega_j(p))} f(p, \omega_o(p), \omega_j(p)) |\cos(\theta_j(p))|\times <br />
&amp; ~~~~~~~~~~~~~\frac{1}{P(A_L(p&rdquo;))}f(p&rdquo;\to p&rsquo; \to p))L(p&rdquo;\to p)G(p&rdquo;, p&rsquo;).
\end{aligned}
$$
Two things should be noted. First, since we assumed $p&rdquo;$ is the closest intersection in direction $\omega_i(p)$, the visibility term is simply $1$. Second, the probability term $P(A_L(p&rdquo;))$ actually cancels a lot of terms in the denominator, as is with Equation (3).</p>

<p>In practice, paths are often constructed in an incremental manner. We sample the first segment from the camera to the scene, and calculate the first bounce image. We then sample the second segment according to the point&rsquo;s BSDF (though which might be inaccurate to sample from since we do not know the other factor $L_i$), calculate the second bounce image and add it to the first bounce image, we then continue the path until the path is lost in void, or has reached the prescribed maximum depth, or terminated by <em>Russian Roulette</em>.
The path so generated has a probability associated, and as before the harmonic average is done to obtain the Monte-Carlo estimate.</p>

<h2 id="2-3-bidirectional-methods">2.3. Bidirectional methods</h2>

<p>Suppose we have traced a path of bounce $n$. The contribution of this path to the $n$-bounce image depends on how probable the light can be well-sampled.
This very last step of light sampling depends on the previously sampled path vertices, which may well be sub-optimal and conribute little to the $n$-bounce image.
Consequently, in tricky lighting conditions, such as a light source hidden behind a cover, or for materials that refract light, conventional path-tracing will take enormous time to converge to a acceptable result.</p>

<p>We briefly describe two approaches that take advantage of the fact that light paths can be reversed, in other words, it is the paths, not directions of the propagation, that matters in the stationary light transport simulation. Path tracing is done in a way that reverses the direction of light. These methods are thus called called <em>bidirectional</em>.</p>

<ul>
<li><em>Bidirectional path tracing</em>  traces paths from the camera and from the light, and connect these paths at intermediate vertices. Paths can be reused and one needs to take into account the many posibility of connecting the paths and use multiple importance sampling. Furhter improvement in sampling strategy leads to the class of methods based on <em>Metropolis-Hasting light transport</em>.</li>
<li><em>Photon mapping</em>  assumes raidiance is smooth and traces rays from the light sources and deposits them at sparse locations in the scene, and paths from the camera are traced and the contribution to the final image is interpolated and calculated from these sparsely cached radiance.</li>
</ul>

<p>Further details and references can be found in the <strong>pbrt</strong> book.</p>

<h1 id="3-image-space-stationary-light-transport">3. Image space stationary light transport</h1>

<p>Leaving from the world of graphics to computer vision, we go in the reverse direction: given one or multiple images, how do they contribute to our knowledge of stationary light transport of the scene being imaged?</p>

<p>From what we know about the linearity of light transport, we have a decomposition of the image $I$ into a sum of images $I_{L_i}$ that are obtained from light sources $L_i$ indexed in $i$. Think of $I$ as a column vector, then we can write this as</p>

<p>$$
I = Tl
$$ (4)
where $l$ is a coefficient vector representing weighted combination of different lights, and $T$ is called the <em>light transport matrix</em> for the image $I$. The collection of light sources is assumed to be fixed. We are interested in the structure of the matrix $T$. Columns of $T$ have a very clear meaning: they are simply the images $I_{L_i}$. It is a more entertaining exercise to think about the meaning of the rows of $T$, and in particular, reversibility of light paths. We will come to this point soon.</p>

<h2 id="3-1-nystr-ouml-m-light-transport">3.1. Nystr&ouml;m light transport</h2>

<p>If we have a good number of light sources, then we expect nearby light sources in space to produce similar effects. In terms of the matrix $T$, this property is translated to &ldquo;nonlinear coherence&rdquo; between columns or rows of $T$. Thus finding the most &ldquo;incoherent&rdquo; portion of the $T$ will help us understand $T$ more than others.</p>

<p>To be more precise, suppose we know $r$ rows $\begin{bmatrix} A &amp; R \end{bmatrix}$ and $c$ columns $\begin{bmatrix} A \ C \end{bmatrix}$ out of $T$. They carry the most information of $T$ if the $r\times c$ submatrix $A$ has the same rank with $T$. If so, the remaining portion of $T$ can be reconstructed:</p>

<p>$$
T = \begin{bmatrix} A &amp; R \ C &amp; CA^{+}R  \end{bmatrix}
$$
where $A^{+}$ denotes the Moore-Penrose pseudoinverse of $A$. If the ranks are not exactly the same, the above is still the best approximation in the sense of Frobenius norm for matrices.
Note that the implicit assumption here is that the matrix $T$ is approximately <em>low-rank</em>.
This property can be enhanced using, such as the <em>kernel trick</em> (Wang et al, Siggraph2009), or neural networks (Ren <em>et al</em>, Siggraph 2015; Xu <em>et al</em>, Siggraph 2018). Once $T$ is reconstructed, the scene can be re-lit under arbitrary lighting coeffcient $l$ from Equation (4).</p>

<p>We must mention how one can actually sample the columns and more importantly rows of the transport matrix $T$. The figure below shows the setting of Wang et al. Here two pairs of camera-projector are used for principled sampling of columns and rows.</p>

<figure>
<img src="figs/sampling_rl.png" alt="sampling_rl" width=400 style="padding-bottom:0.5em;"/> 
<figcaption>Fig.4 Sampling columns and rows of the light transport matrix. (a) Photograph of the scene in column sampling. (b, c) Two column sampling images. (d) Photograph of the scene in row sampling. (e, f) Two row sampling images. The corresponding pixels
are marked in (c). The figure is taken from the paper of Wang <i>et al</i>., Siggraph2009.</figcaption>
</figure>

<h2 id="3-2-light-transport-probing">3.2. Light transport probing</h2>

<p>It is possible to gain more insight into the light transport matrix, if not only do we have control over the light sources, for example using a beam projector, but also <em>control over how light sensing is made</em>.
The technique, developed by O&rsquo;Toole <em>et al</em>. (Siggraph 2012) called <em>primal-dual transport probing</em>, is capable of separating direct and indirect illumination.
This is one of the major differences from Nyst&ouml;m relighting technique described previously.</p>

<p>To probe the light transport matrix means to get a new image $I&rsquo;$ via a probing matrix $\Pi$</p>

<p>$$
I&rsquo; = \Pi \odot T \mathbf{1}
$$
where $\odot$ means element-wise multiplication and $\mathbf{1}$ means a vector of all ones. This formulation certainly includes Equaiton (4) as a special case.
Intuitively, we can think of it as masking-out certain pixels for each light source and finally add-up them together.</p>

<p>Of course, there&rsquo;s price to pay: in order to know what to probe in the light transport matrix, one needs to know the meaning of each entry in it, that is, which light contributes to which pixel. This is often achieved by restricting the spatial relation between the projector and the camera, as shown in Figure 5.</p>

<figure>
<img src="figs/proj_cam_arr.png" alt="proj_cam_arr" width=400 style="padding-bottom:0.5em;"/> 
<figcaption>Fig.5 Projector-camera spatial setup. The <b> coaxial </b> arrangement (a) in O'Toole _et al_. essentially aligned each point light source with each pixel. In <b>non-coaxial</b> stereo arrangement (b), the light-pixel relation can be trickier, which need resolving to multi-view geometry. Under the coaxial arrangement, types of light paths shown in (c, d) are easier to identify. This figure is taken from the paper of O'Toole <i>et al</i>., Siggraph2012.</figcaption>
</figure>

<p>Under coaxial arrangement, the entries of the light transport matrix can be classified into corresponding light path types. For simplicity, we consider the light transport situation of a one-dimentional slice of the image. For 2D images the light transport operator is better illustrated as a 3D tensor.</p>

<table>
<thead>
<tr>
<th>Entries in $T$</th>
<th align="center">Type of light path</th>
</tr>
</thead>

<tbody>
<tr>
<td>Diagonal</td>
<td align="center">Direct and back-scattering paths</td>
</tr>

<tr>
<td>All off-diagonal</td>
<td align="center">All indirect light paths</td>
</tr>

<tr>
<td>Far off-diagonal</td>
<td align="center">Long-range indirect</td>
</tr>

<tr>
<td>Near off-diagonal</td>
<td align="center">Short-range indirect (mainly)</td>
</tr>
</tbody>
</table>

<p>Distributional properties within the matrix $T$ also tells us about the distribution of light paths. For example, diffusive reflections should result in non-zero entries distributed over many entries, while glossy and specular reflections should result in more concentration in smaller amount of entries.</p>

<p>To probe the transport matrix, not only do we need to precisely control the point light sources, but also precisely control pixel response to light. In principle one could record pixelwise response to each point light source, but that would be too expensive to do, especially when we only want the resulting image for a particular probing. More generally, one can illuminate the scene with a specific light pattern $l$, and record the image with specific pixel masked, with mask $m$. The obtained image $I_{m,l}$ can be mathematically expressed as</p>

<p>$$
I_{m,l} = M T l =  (m ~l^T) \odot T ~\mathbf{1}
$$
where the $i$-th row of $M$ is $M_i = (m^{T})$. Thus we can probe the light transport with matrix $\Pi$ by decomposing $\Pi$ into a sum of <em>rank one</em> matrices:</p>

<p>$$
\Pi = \sum_k m<em>k ~l^{T}</em>{k}
$$
that is, a sequence of light patterns and masks.
In the paper of O&rsquo;Toole <em>et al</em> (2012)., they used a <em>stochastic</em> estimation, which is an amusing technique in itself developed by Hutchinson (1990) and Bekas <em>et al.</em> (2007) to estimate entries in a matrix. In short, instead of recording one image for each light source with masked pixels, the camera will record the desired image over a single exposure period, during which random <em>Rademacher light</em> $\mathbf{i}_k$ are projected to the scene and corresponding pixels on the sensor are masked according to $\Pi~\mathbf{i}_k$ . Within the exposure period, we have $k=1, &hellip; K$ different light patterns together with masks integrated. The result will converge as $K \to \infty$. Below is a very illustrative example of what their approach is capable of:</p>

<figure>
<img src="figs/primal_dual_probing.png" alt="primal_dual_probing" width=700 style="padding-bottom:0.5em;"/> 
<figcaption>Fig.6 Light transport probing. (d,f) also uses the method of Nayar (2006) to futher decompose the indirect component. This figure is taken from the paper of O'Toole <i>et al</i>., Siggraph2012.</figcaption>
</figure>

<p>Of course, if not at all necessary, stochastic estimation is not really that desirable. In a 2014 SIGGRAH paper, O&rsquo;Toole <em>et al.</em> proposed a new method based on matrix factorization, by optimizing the efficiency of light patterns and masks.</p>

<h2 id="3-3-transient-light-transport">3.3. Transient light transport</h2>

<p>So far we have only considered light transportation in stationary state, meaning that the transportation has reached equilibrium, as per the rendering equaiton (1).
We can add in an extra time dimension and to render light in <em>transient</em> state, and thus the <em>transient rendering</em>. The idea is clearer in the path integral framework, we can simply look at the length of each light path, group them according to length and accordingly, time of flight of a specific photon.</p>

<p>Reversely, if we know the time of flight, we then know the length of the corresponding light path. The fact that in most cases, direct illumination contributes most in the image formation, explains the use of <em>time-of-flight (ToF) depth sensing</em>. The effect from ndirect illumination on depth sensing is called <em>multi-path interference</em>.</p>

<p>The image space light transport (4) can also unfold along the time dimension. Now it reads</p>

<p>$$
I(\delta t) = T(\delta t)~l
$$
where $\delta t$ denotes the time interval starting from the instant when an <em>temporal impulse</em> of light with spatial pattern $l$ happens.</p>

<p>Suppose we have a temporally varing light $l(\tau)$. Because of translational symmetry in time dimesion, we have the resulting cummulative image at pixel $p$ up to time $t$ as a superposition of above images with different $\delta t$:</p>

<p>$$
I(t,p) = \int<em>{0}^{\infty} \sum</em>{q} T(\tau, p, q)~l(t-\tau, q) d\tau
$$ (5)
where of cource $l(\tau)=0$ if $\tau&lt;0$. We may extend the integral to the real line by setting $T(\tau, \cdot, \cdot)=0$ if $\tau&lt;0$.</p>

<p>Now consider a sinusiodal light $l^{\omega}$ at frequency $\omega$. This is best done by using complex numbers in $l$ and $I$. Pluging it in (5), and using the fact that Fourier transform turns convolution into multiplication, we have</p>

<p>$$
I^{\omega}(p) = \sum_{q} \widehat{T}(\omega, p, q)~ l^{\omega}(q)
$$ (6)
where $\widehat{T}(\omega, \cdot, \cdot)$ denotes the time-dimension Fourier transform of $T(\tau, \cdot, \cdot)$. Note that Equation (6) is in the same form with Equation (4). We now have a familiar form of image space single frequency light tranportation</p>

<p>$$
I^{\omega} = T^{\omega} ~l^{\omega}.
$$
O&rsquo;Toole <em>et al.</em> take advantage of this obsevation in their another Siggraph 2014 paper, using the same technique in O&rsquo;Toole <em>et al.</em> Siggraph 2012 &amp; 2014 described in the previous section to deal with transient light transport. It is worth mentioning that the depth sensing by probing the direct and retro-reflection component is a very clever way of reducing multi-path interference in optical domain. Further development of this approach include <em>eipolar ToF imaging</em>, by Apreeth <em>et al.</em> in Siggraph 2017.</p>

<figure>
<img src="figs/ToF_probe.png" alt="ToF_probe" width=800 style="padding-bottom:0.5em;"/> 
<figcaption>Fig.7 (a) The phase of conventionally-acquired PMD photos. (b) The phase of direct/retro-reflective PMD photos returned by Algorithm 3. (c-d) Views of the 3D meshes computed from (a) and (b), respectively. (e) Plots of the x- and z-coordinates for a slice of each scene, computed from the conventional (blue) and the direct/retro-reflective (red) phases. Observe that the base of the conventionally-acquired bowl protrudes through the back wall by about 5 cm; the pages of the conventionally-acquired book appear curved; the corner of the room in the conventionally-acquired David scene is rounded, and the caustic paths illuminating the room’s right wall produce a 2 to 3 cm offset in depth values. None of these artifacts appear in (b) or (d). This figure is taken from the paper of O'Toole <i>et al</i>., Siggraph2014.</figcaption>
</figure>

    </article>

    
<ul class="article-share">
  <li>
    <a href="https://twitter.com/share" class="twitter-share-button">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </li>
  <li>
    <div class="fb-share-button" data-href="http://example.org/post/light-transport/" data-layout="button_count" data-action="like" data-size="small" data-show-faces="true" data-share="true"></div>
    <div id="fb-root"></div>
    <script>(function(d, s, id) {
      var js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
      js = d.createElement(s); js.id = id;
      js.src = "//connect.facebook.net/ja_JP/sdk.js#xfbml=1&version=v2.10";
      fjs.parentNode.insertBefore(js, fjs);
    }(document, 'script', 'facebook-jssdk'));</script>
  </li>
  <li>
    <a href="http://b.hatena.ne.jp/entry/" class="hatena-bookmark-button" data-hatena-bookmark-layout="basic-label-counter" data-hatena-bookmark-lang="en" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
  </li>
  <li>
    <a data-pocket-label="pocket" data-pocket-count="horizontal" class="pocket-btn" data-lang="en"></a>
    <script>!function(d,i){if(!d.getElementById(i)){var j=d.createElement("script");j.id=i;j.src="https://widgets.getpocket.com/v1/j/btn.js?v=1";var w=d.getElementById(i);d.body.appendChild(j);}}(document,"pocket-btn-js");</script>
  </li>
</ul>


    <ul class="pager article-pager">
      <li class="pager-newer pager-noitem">&lt; Newer</li>
      <li class="pager-older pager-noitem">Older &gt;</li>
    </ul>
  </div>


<div class="site-footer">
  <div class="copyright"></div>
  <ul class="site-footer-items">
  </ul>
  
</div>



</body>
</html>
